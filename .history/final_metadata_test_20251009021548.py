#!/usr/bin/env python3
"""
FINAL COMPREHENSIVE METADATA TEST
Tests all aspects of metadata handling for both FAISS and ChromaDB
"""
import requests
import json
import pandas as pd
import numpy as np
import time
from backend import (
    store_faiss_enhanced, 
    store_chroma_enhanced,
    query_faiss_with_metadata,
    create_metadata_index,
    apply_metadata_filter,
    document_based_chunking_enhanced
)

def test_backend_metadata_functions():
    """Test all backend metadata functions"""
    print('üîß TESTING BACKEND METADATA FUNCTIONS')
    print('=' * 60)
    
    # Create test data
    chunks = [
        "The system is running smoothly and efficiently in New York office",
        "We are analyzing the data carefully and thoroughly in Los Angeles branch",
        "The machines are working better than expected in Chicago facility",
        "Processing information automatically and accurately in Seattle center",
        "The results show significant improvement over time in Miami location"
    ]
    
    embeddings = np.random.rand(5, 384)
    
    # Rich metadata with various data types
    metadata = [
        {
            'chunk_id': '0', 'key_value': 'NYC', 'score_min': 85.5, 'score_mean': 85.5, 'score_max': 85.5,
            'city_mode': 'NYC', 'category_mode': 'A', 'department_mode': 'IT', 'priority_mode': 'High'
        },
        {
            'chunk_id': '1', 'key_value': 'LA', 'score_min': 92.3, 'score_mean': 92.3, 'score_max': 92.3,
            'city_mode': 'LA', 'category_mode': 'B', 'department_mode': 'Analytics', 'priority_mode': 'Medium'
        },
        {
            'chunk_id': '2', 'key_value': 'Chicago', 'score_min': 78.1, 'score_mean': 78.1, 'score_max': 78.1,
            'city_mode': 'Chicago', 'category_mode': 'A', 'department_mode': 'IT', 'priority_mode': 'Low'
        },
        {
            'chunk_id': '3', 'key_value': 'Seattle', 'score_min': 88.7, 'score_mean': 88.7, 'score_max': 88.7,
            'city_mode': 'Seattle', 'category_mode': 'C', 'department_mode': 'Operations', 'priority_mode': 'High'
        },
        {
            'chunk_id': '4', 'key_value': 'Miami', 'score_min': 91.2, 'score_mean': 91.2, 'score_max': 91.2,
            'city_mode': 'Miami', 'category_mode': 'B', 'department_mode': 'Analytics', 'priority_mode': 'Medium'
        }
    ]
    
    print('Test data:')
    print(f'Chunks: {len(chunks)}')
    print(f'Embeddings: {embeddings.shape}')
    print(f'Metadata: {len(metadata)} entries')
    print()
    
    # Test 1: Metadata Index Creation
    print('TEST 1: Metadata Index Creation')
    print('-' * 40)
    try:
        metadata_index = create_metadata_index(metadata)
        print('‚úÖ Metadata index created successfully')
        print(f'Index keys: {list(metadata_index.keys())}')
        print(f'Sample index structure:')
        for key, values in list(metadata_index.items())[:3]:
            print(f'  {key}: {len(values)} unique values')
    except Exception as e:
        print(f'‚ùå Metadata index creation failed: {e}')
        return
    
    print()
    
    # Test 2: Metadata Filtering
    print('TEST 2: Metadata Filtering')
    print('-' * 40)
    
    filter_tests = [
        {'city_mode': 'NYC'},
        {'category_mode': 'A'},
        {'department_mode': 'IT'},
        {'priority_mode': 'High'},
        {'city_mode': 'NYC', 'department_mode': 'IT'},
        {'category_mode': 'A', 'priority_mode': 'High'},
        {'score_mean': 85.5},
        {'city_mode': 'Miami', 'category_mode': 'B', 'priority_mode': 'Medium'}
    ]
    
    for i, filter_dict in enumerate(filter_tests):
        print(f'Filter {i+1}: {filter_dict}')
        try:
            matching_indices = apply_metadata_filter(metadata_index, filter_dict)
            print(f'  Matching indices: {matching_indices}')
            print(f'  Matches: {len(matching_indices)}')
        except Exception as e:
            print(f'  Error: {e}')
        print()
    
    # Test 3: FAISS Storage with Metadata
    print('TEST 3: FAISS Storage with Metadata')
    print('-' * 40)
    try:
        faiss_result = store_faiss_enhanced(chunks, embeddings, metadata)
        print('‚úÖ FAISS storage with metadata successful')
        print(f'Result type: {faiss_result["type"]}')
        print(f'Metadata index created: {"metadata_index" in faiss_result}')
        print(f'Total vectors: {faiss_result["data"]["total_vectors"]}')
    except Exception as e:
        print(f'‚ùå FAISS storage failed: {e}')
        return
    
    print()
    
    # Test 4: FAISS Query with Metadata Filtering
    print('TEST 4: FAISS Query with Metadata Filtering')
    print('-' * 40)
    
    query_tests = [
        {'filter': {}, 'description': 'No filter'},
        {'filter': {'city_mode': 'NYC'}, 'description': 'Filter by city (NYC)'},
        {'filter': {'department_mode': 'IT'}, 'description': 'Filter by department (IT)'},
        {'filter': {'priority_mode': 'High'}, 'description': 'Filter by priority (High)'},
        {'filter': {'city_mode': 'LA', 'category_mode': 'B'}, 'description': 'Multiple filters (LA + B)'}
    ]
    
    for test in query_tests:
        print(f'{test["description"]}:')
        try:
            query_embedding = np.random.rand(1, 384).astype("float32")
            results = query_faiss_with_metadata(
                faiss_result["index"], 
                faiss_result["data"], 
                query_embedding, 
                k=3, 
                metadata_filter=test["filter"]
            )
            print(f'  Results: {len(results)}')
            if results:
                print(f'  Top result metadata: {results[0].get("metadata", {})}')
        except Exception as e:
            print(f'  Error: {e}')
        print()
    
    print('=' * 60)

def test_api_metadata_integration():
    """Test complete API metadata integration"""
    print('üåê TESTING API METADATA INTEGRATION')
    print('=' * 60)
    
    base_url = "http://localhost:8000"
    
    # Create comprehensive test data
    test_data = {
        'text': [
            'The system is running smoothly and efficiently in New York office with high performance metrics',
            'We are analyzing the data carefully and thoroughly in Los Angeles branch with advanced analytics',
            'The machines are working better than expected in Chicago facility with optimal efficiency',
            'Processing information automatically and accurately in Seattle center with real-time updates',
            'The results show significant improvement over time in Miami location with enhanced reporting'
        ],
        'score': [85.5, 92.3, 78.1, 88.7, 91.2],
        'city': ['NYC', 'LA', 'Chicago', 'Seattle', 'Miami'],
        'category': ['A', 'B', 'A', 'C', 'B'],
        'department': ['IT', 'Analytics', 'IT', 'Operations', 'Analytics'],
        'priority': ['High', 'Medium', 'Low', 'High', 'Medium'],
        'budget': [100000, 150000, 80000, 120000, 130000]
    }
    
    df = pd.DataFrame(test_data)
    
    print('Test DataFrame:')
    print(df)
    print()
    
    # Test both storage types
    storage_configs = [
        {
            'name': 'ChromaDB',
            'storage_type': 'chroma',
            'description': 'Native metadata support'
        },
        {
            'name': 'FAISS',
            'storage_type': 'faiss',
            'description': 'Enhanced metadata support'
        }
    ]
    
    for config in storage_configs:
        print(f'TESTING {config["name"]} ({config["description"]})')
        print('-' * 50)
        
        # Step 1: Preprocess
        print('Step 1: Preprocessing')
        try:
            files = {'file': ('test.csv', df.to_csv(index=False), 'text/csv')}
            response = requests.post(f"{base_url}/deep_config/preprocess", files=files)
            if response.status_code != 200:
                print(f'‚ùå Preprocessing failed: {response.text}')
                continue
            print('‚úÖ Preprocessing successful')
        except Exception as e:
            print(f'‚ùå Preprocessing error: {e}')
            continue
        
        # Step 2: Document-based chunking with metadata
        print('Step 2: Document-based chunking with metadata')
        try:
            chunk_data = {
                'chunk_method': 'document',
                'key_column': 'city',  # Group by city
                'token_limit': 1000,
                'store_metadata': 'true',
                'numeric_columns': 2,  # Include score and budget
                'categorical_columns': 3  # Include category, department, priority
            }
            response = requests.post(f"{base_url}/deep_config/chunk", data=chunk_data)
            if response.status_code != 200:
                print(f'‚ùå Chunking failed: {response.text}')
                continue
            
            result = response.json()
            print(f'‚úÖ Chunking successful: {result.get("total_chunks", 0)} chunks')
            print(f'Metadata enabled: {result.get("metadata_enabled", False)}')
            
            # Show sample metadata
            metadata = result.get('metadata', [])
            if metadata:
                print('Sample metadata:')
                for i, meta in enumerate(metadata[:2]):
                    print(f'  Chunk {i+1}: {meta}')
        except Exception as e:
            print(f'‚ùå Chunking error: {e}')
            continue
        
        # Step 3: Embedding
        print('Step 3: Embedding')
        try:
            embed_data = {'model_name': 'all-MiniLM-L6-v2'}
            response = requests.post(f"{base_url}/deep_config/embed", data=embed_data)
            if response.status_code != 200:
                print(f'‚ùå Embedding failed: {response.text}')
                continue
            print('‚úÖ Embedding successful')
        except Exception as e:
            print(f'‚ùå Embedding error: {e}')
            continue
        
        # Step 4: Storage
        print(f'Step 4: Storage in {config["name"]}')
        try:
            store_data = {'storage_type': config['storage_type']}
            response = requests.post(f"{base_url}/deep_config/store", data=store_data)
            if response.status_code != 200:
                print(f'‚ùå Storage failed: {response.text}')
                continue
            
            result = response.json()
            print(f'‚úÖ Storage successful: {result.get("total_vectors", 0)} vectors')
            print(f'Storage type: {result.get("storage_type", "unknown")}')
        except Exception as e:
            print(f'‚ùå Storage error: {e}')
            continue
        
        # Step 5: Comprehensive metadata filtering tests
        print('Step 5: Comprehensive metadata filtering tests')
        
        filter_tests = [
            {
                'name': 'No filter',
                'query': 'system performance',
                'k': 5,
                'metadata_filter': '{}'
            },
            {
                'name': 'Filter by city (NYC)',
                'query': 'system performance',
                'k': 3,
                'metadata_filter': '{"city_mode": "nyc"}'
            },
            {
                'name': 'Filter by department (IT)',
                'query': 'machines working',
                'k': 3,
                'metadata_filter': '{"department_mode": "it"}'
            },
            {
                'name': 'Filter by priority (High)',
                'query': 'results improvement',
                'k': 3,
                'metadata_filter': '{"priority_mode": "high"}'
            },
            {
                'name': 'Filter by score range',
                'query': 'data analysis',
                'k': 3,
                'metadata_filter': '{"score_mean": 92.3}'
            },
            {
                'name': 'Filter by budget range',
                'query': 'processing information',
                'k': 3,
                'metadata_filter': '{"budget_mean": 150000}'
            },
            {
                'name': 'Multiple filters (NYC + IT)',
                'query': 'system efficiency',
                'k': 3,
                'metadata_filter': '{"city_mode": "nyc", "department_mode": "it"}'
            },
            {
                'name': 'Complex filters (High priority + Category A)',
                'query': 'optimal performance',
                'k': 3,
                'metadata_filter': '{"priority_mode": "high", "category_mode": "a"}'
            }
        ]
        
        for test in filter_tests:
            print(f'\\n  {test["name"]}:')
            print(f'    Query: {test["query"]}')
            print(f'    Filter: {test["metadata_filter"]}')
            
            try:
                query_data = {
                    'query': test['query'],
                    'k': test['k'],
                    'metadata_filter': test['metadata_filter']
                }
                response = requests.post(f"{base_url}/retrieve_with_metadata", data=query_data)
                
                if response.status_code == 200:
                    result = response.json()
                    print(f'    Results: {result.get("total_results", 0)}')
                    print(f'    Filter applied: {result.get("metadata_filter_applied", False)}')
                    print(f'    Store type: {result.get("store_type", "unknown")}')
                    
                    # Show top result metadata
                    results = result.get('results', [])
                    if results:
                        top_result = results[0]
                        print(f'    Top result similarity: {top_result.get("similarity", 0):.3f}')
                        print(f'    Top result metadata: {top_result.get("metadata", {})}')
                else:
                    print(f'    Error: {response.text}')
                    
            except Exception as e:
                print(f'    Error: {e}')
        
        print('=' * 60)
        print()

def test_frontend_metadata_flow():
    """Test frontend metadata flow simulation"""
    print('üé® TESTING FRONTEND METADATA FLOW')
    print('=' * 60)
    
    base_url = "http://localhost:8000"
    
    # Simulate frontend metadata selection flow
    print('Simulating frontend metadata selection flow...')
    
    # Test data
    test_data = {
        'text': [
            'The system is running smoothly in New York',
            'We are analyzing data carefully in Los Angeles',
            'The machines are working efficiently in Chicago'
        ],
        'score': [85.5, 92.3, 78.1],
        'city': ['NYC', 'LA', 'Chicago'],
        'category': ['A', 'B', 'A'],
        'department': ['IT', 'Analytics', 'IT']
    }
    
    df = pd.DataFrame(test_data)
    
    # Simulate UI selections
    ui_selections = [
        {
            'storage_choice': 'ChromaDB',
            'store_metadata': True,
            'numeric_columns': 1,  # score
            'categorical_columns': 2,  # city, category
            'description': 'ChromaDB with metadata (score, city, category)'
        },
        {
            'storage_choice': 'FAISS',
            'store_metadata': True,
            'numeric_columns': 1,  # score
            'categorical_columns': 2,  # city, category
            'description': 'FAISS with metadata (score, city, category)'
        },
        {
            'storage_choice': 'ChromaDB',
            'store_metadata': False,
            'numeric_columns': 0,
            'categorical_columns': 0,
            'description': 'ChromaDB without metadata'
        },
        {
            'storage_choice': 'FAISS',
            'store_metadata': False,
            'numeric_columns': 0,
            'categorical_columns': 0,
            'description': 'FAISS without metadata'
        }
    ]
    
    for selection in ui_selections:
        print(f'\\nTesting: {selection["description"]}')
        print('-' * 50)
        
        # Step 1: Preprocess
        try:
            files = {'file': ('test.csv', df.to_csv(index=False), 'text/csv')}
            response = requests.post(f"{base_url}/deep_config/preprocess", files=files)
            if response.status_code != 200:
                print(f'‚ùå Preprocessing failed: {response.text}')
                continue
        except Exception as e:
            print(f'‚ùå Preprocessing error: {e}')
            continue
        
        # Step 2: Chunking with UI selections
        try:
            chunk_data = {
                'chunk_method': 'document',
                'key_column': 'city',
                'token_limit': 1000,
                'store_metadata': str(selection['store_metadata']).lower(),
                'numeric_columns': selection['numeric_columns'],
                'categorical_columns': selection['categorical_columns']
            }
            response = requests.post(f"{base_url}/deep_config/chunk", data=chunk_data)
            if response.status_code != 200:
                print(f'‚ùå Chunking failed: {response.text}')
                continue
            
            result = response.json()
            print(f'‚úÖ Chunking: {result.get("total_chunks", 0)} chunks, metadata: {result.get("metadata_enabled", False)}')
        except Exception as e:
            print(f'‚ùå Chunking error: {e}')
            continue
        
        # Step 3: Embedding
        try:
            embed_data = {'model_name': 'all-MiniLM-L6-v2'}
            response = requests.post(f"{base_url}/deep_config/embed", data=embed_data)
            if response.status_code != 200:
                print(f'‚ùå Embedding failed: {response.text}')
                continue
        except Exception as e:
            print(f'‚ùå Embedding error: {e}')
            continue
        
        # Step 4: Storage
        try:
            storage_type = 'chroma' if selection['storage_choice'] == 'ChromaDB' else 'faiss'
            store_data = {'storage_type': storage_type}
            response = requests.post(f"{base_url}/deep_config/store", data=store_data)
            if response.status_code != 200:
                print(f'‚ùå Storage failed: {response.text}')
                continue
            
            result = response.json()
            print(f'‚úÖ Storage: {result.get("total_vectors", 0)} vectors in {result.get("storage_type", "unknown")}')
        except Exception as e:
            print(f'‚ùå Storage error: {e}')
            continue
        
        # Step 5: Test retrieval
        if selection['store_metadata']:
            try:
                query_data = {
                    'query': 'system performance',
                    'k': 3,
                    'metadata_filter': '{"city_mode": "nyc"}'
                }
                response = requests.post(f"{base_url}/retrieve_with_metadata", data=query_data)
                if response.status_code == 200:
                    result = response.json()
                    print(f'‚úÖ Retrieval: {result.get("total_results", 0)} results with metadata filtering')
                else:
                    print(f'‚ùå Retrieval failed: {response.text}')
            except Exception as e:
                print(f'‚ùå Retrieval error: {e}')
        else:
            print('‚úÖ No metadata filtering test (metadata disabled)')
    
    print('=' * 60)

def test_performance_benchmarks():
    """Test performance benchmarks for metadata operations"""
    print('‚ö° TESTING PERFORMANCE BENCHMARKS')
    print('=' * 60)
    
    # Create larger dataset for performance testing
    n_chunks = 1000
    chunks = [f"This is chunk {i} with some text content for performance testing" for i in range(n_chunks)]
    embeddings = np.random.rand(n_chunks, 384)
    
    # Create metadata with various patterns
    metadata = []
    for i in range(n_chunks):
        metadata.append({
            'chunk_id': str(i),
            'key_value': str(i % 10),  # 10 different key values
            'score_min': 70 + (i % 30),  # 30 different scores
            'score_mean': 70 + (i % 30),
            'score_max': 70 + (i % 30),
            'city_mode': ['NYC', 'LA', 'Chicago', 'Miami', 'Seattle'][i % 5],  # 5 cities
            'category_mode': ['A', 'B', 'C', 'D', 'E'][i % 5],  # 5 categories
            'department_mode': ['IT', 'Analytics', 'Operations', 'Sales', 'Marketing'][i % 5]  # 5 departments
        })
    
    print(f'Performance test dataset: {n_chunks} chunks with rich metadata')
    
    # Test metadata index creation performance
    print('\\nTesting metadata index creation performance...')
    start_time = time.time()
    metadata_index = create_metadata_index(metadata)
    index_time = time.time() - start_time
    
    print(f'‚úÖ Metadata index creation: {index_time:.4f} seconds')
    print(f'Index size: {len(metadata_index)} keys')
    
    # Test filtering performance
    print('\\nTesting filtering performance...')
    filter_tests = [
        {'city_mode': 'NYC'},
        {'category_mode': 'A'},
        {'department_mode': 'IT'},
        {'city_mode': 'NYC', 'category_mode': 'A'},
        {'city_mode': 'LA', 'department_mode': 'Analytics'},
        {'score_mean': 85}
    ]
    
    for filter_dict in filter_tests:
        start_time = time.time()
        matching_indices = apply_metadata_filter(metadata_index, filter_dict)
        filter_time = time.time() - start_time
        
        print(f'Filter {filter_dict}: {len(matching_indices)} matches in {filter_time:.6f}s')
    
    # Test FAISS storage performance
    print('\\nTesting FAISS storage performance...')
    start_time = time.time()
    faiss_result = store_faiss_enhanced(chunks, embeddings, metadata)
    storage_time = time.time() - start_time
    
    print(f'‚úÖ FAISS storage: {storage_time:.4f} seconds')
    print(f'Total vectors: {faiss_result["data"]["total_vectors"]}')
    
    # Test query performance
    print('\\nTesting query performance...')
    query_embedding = np.random.rand(1, 384).astype("float32")
    
    # No filter
    start_time = time.time()
    results = query_faiss_with_metadata(faiss_result["index"], faiss_result["data"], query_embedding, k=10)
    query_time_no_filter = time.time() - start_time
    
    # With filter
    start_time = time.time()
    results = query_faiss_with_metadata(faiss_result["index"], faiss_result["data"], query_embedding, k=10, metadata_filter={'city_mode': 'NYC'})
    query_time_with_filter = time.time() - start_time
    
    print(f'‚úÖ Query without filter: {query_time_no_filter:.6f}s')
    print(f'‚úÖ Query with filter: {query_time_with_filter:.6f}s')
    print(f'Filter overhead: {((query_time_with_filter - query_time_no_filter) / query_time_no_filter * 100):.2f}%')
    
    print('=' * 60)

if __name__ == "__main__":
    print('üöÄ STARTING FINAL COMPREHENSIVE METADATA TEST')
    print('=' * 80)
    print('This test covers:')
    print('‚Ä¢ Backend metadata functions (indexing, filtering, storage)')
    print('‚Ä¢ API integration (preprocessing, chunking, embedding, storage)')
    print('‚Ä¢ Frontend flow simulation (UI selections, parameter passing)')
    print('‚Ä¢ Performance benchmarks (large datasets, filtering speed)')
    print('‚Ä¢ Both ChromaDB and FAISS storage backends')
    print('‚Ä¢ User-selected and auto-generated metadata')
    print('=' * 80)
    print()
    
    try:
        # Test 1: Backend functions
        test_backend_metadata_functions()
        
        # Test 2: API integration
        test_api_metadata_integration()
        
        # Test 3: Frontend flow
        test_frontend_metadata_flow()
        
        # Test 4: Performance benchmarks
        test_performance_benchmarks()
        
        print('üéâ FINAL COMPREHENSIVE METADATA TEST COMPLETED SUCCESSFULLY!')
        print('=' * 80)
        print('‚úÖ All backend metadata functions working correctly')
        print('‚úÖ API integration with both ChromaDB and FAISS successful')
        print('‚úÖ Frontend metadata flow simulation successful')
        print('‚úÖ Performance benchmarks within acceptable ranges')
        print('‚úÖ User-selected and auto-generated metadata working')
        print('‚úÖ Metadata filtering and retrieval working for both storage types')
        print('=' * 80)
        
    except Exception as e:
        print(f'‚ùå FINAL TEST FAILED: {e}')
        print('Make sure the FastAPI server is running: uvicorn main:app --reload')
