# 📦 Chunking Optimizer v1.0

A powerful text processing and semantic search application that converts CSV files and database tables into searchable vector embeddings with advanced preprocessing, chunking, and retrieval capabilities.

## 🚀 Features

### **Core Capabilities**
- **Multiple Data Sources**: Import from CSV files or database tables (MySQL/PostgreSQL)
- **Three Processing Modes**: Fast, Config-1, and Deep Config with varying levels of control
- **Advanced Preprocessing**: Smart suggestions, type conversion, null handling, text normalization
- **Flexible Chunking**: Fixed, recursive, semantic, and document-based chunking strategies
- **Vector Search**: Semantic similarity search using FAISS or ChromaDB
- **Large File Support**: 3GB+ file processing with disk streaming and batch processing
- **Export Options**: Download processed chunks and embeddings in multiple formats
- **Real-time Monitoring**: Process tracking, system information, and performance metrics

### **Enhanced Deep Config Mode**
- **Step-by-Step API Integration**: 9-step guided process with individual API endpoints
- **Smart Preprocessing**: AI-powered suggestions for data type conversion and null handling
- **Advanced Text Processing**: Stopwords removal, lemmatization, and stemming
- **Metadata Management**: Intelligent selection and storage of metadata for filtering
- **Parallel Processing**: Turbo mode for 2-3x faster processing
- **Interactive Interface**: Real-time previews and progress tracking
- **Download Intermediate Results**: CSV/JSON downloads after each processing step

### **Config-1 Mode Enhancements**
- **Document-Based Chunking**: Added document chunking method with key column grouping
- **Retrieval Metric Selection**: Choose between cosine, dot product, and euclidean similarity
- **Removed Null Handling**: Streamlined interface focusing on core functionality
- **Updated Download Formats**: CSV for chunks, JSON for embeddings
- **Database Import Support**: Full database integration with connection testing

## 🏗️ Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │    Backend      │    │   Processing    │
│   (Streamlit)   │◄──►│   (FastAPI)     │◄──►│   Pipeline      │
│                 │    │                 │    │                 │
│ • File Upload   │    │ • API Endpoints │    │ • Preprocessing │
│ • DB Connection │    │ • CSV Processing│    │ • Chunking      │
│ • Search UI     │    │ • DB Import     │    │ • Embeddings    │
│ • Results       │    │ • Vector Storage│    │ • Vector DB     │
│ • Export        │    │ • Export        │    │ • Retrieval     │
│ • Step-by-Step  │    │ • State Mgmt    │    │ • State Persist │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## 📋 Prerequisites

- **Python 3.8+**
- **Memory**: 4GB+ RAM recommended for large files
- **Storage**: 2GB+ free space for vector databases
- **Optional**: MySQL/PostgreSQL for database import

## 🛠️ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/shanttoosh/streamlit_chunking.git
cd streamlit_chunking
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Install Optional Dependencies
```bash
# For database connectivity
pip install psycopg2-binary mysql-connector-python

# For advanced text processing
pip install spacy
python -m spacy download en_core_web_sm

# For NLTK features
pip install nltk
python -c "import nltk; nltk.download('punkt'); nltk.download('popular')"
```

### 4. Start the Application
```bash
# Terminal 1 - Backend API
uvicorn main:app --reload --port 8000

# Terminal 2 - Frontend UI
streamlit run app.py
```

### 5. Access the Application
- **Frontend**: http://localhost:8501
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs

## 📊 Usage Guide

### **Fast Mode** ⚡
**Best for**: Quick processing with good results
- Auto-optimized pipeline with semantic clustering
- paraphrase-MiniLM-L6-v2 embedding model
- FAISS storage for fast retrieval
- Turbo mode enabled by default
- **Database Import**: Full support for MySQL/PostgreSQL

### **Config-1 Mode** ⚙️
**Best for**: Balanced control and performance
- **4 Chunking Methods**: Fixed, Recursive, Semantic, Document
- **Retrieval Metrics**: Cosine, Dot Product, Euclidean similarity
- **Multiple Models**: Local and OpenAI embedding options
- **Storage Options**: FAISS or ChromaDB with metric compatibility
- **Database Import**: Complete database integration
- **Download Formats**: CSV chunks, JSON embeddings

### **Deep Config Mode** 🔬
**Best for**: Maximum control and quality
- **9-Step API Workflow**:
  1. **File Upload/DB Import** - CSV upload or database connection with data preview
  2. **Default Preprocessing** - Header validation and normalization via API
  3. **Type Conversion** - Smart suggestions with interactive interface via API
  4. **Null Handling** - Advanced null strategy selection via API
  5. **Stop Words Removal** - Optional text cleaning via API
  6. **Text Normalization** - Lemmatization/stemming options via API
  7. **Metadata Selection** - ChromaDB metadata configuration
  8. **Chunking** - 4 chunking methods with progress tracking via API
  9. **Embedding & Storage** - Model selection, storage, and retrieval via API

- **Step-by-Step Downloads**: 
  - Preprocessed CSV after text normalization
  - Chunks CSV after chunking process
  - Embeddings JSON after embedding generation

## 🔧 Processing Modes Comparison

| Feature | Fast Mode | Config-1 Mode | Deep Config Mode |
|---------|-----------|---------------|------------------|
| **Preprocessing** | Auto | Basic | Advanced with smart suggestions |
| **Type Conversion** | None | Manual | AI-powered suggestions |
| **Null Handling** | Auto drop | Removed | Advanced strategies |
| **Text Processing** | None | Basic | Stopwords, lemmatization, stemming |
| **Chunking Methods** | Semantic only | 4 methods | 4 methods with metadata |
| **Embedding Models** | 1 model | 3 models | 3 models + OpenAI |
| **Storage Options** | FAISS only | FAISS/ChromaDB | FAISS/ChromaDB with metadata |
| **Retrieval Metrics** | Fixed | 3 options | 3 options |
| **Database Import** | ✅ | ✅ | ✅ |
| **API Integration** | Single call | Single call | Step-by-step |
| **Performance** | Fastest | Balanced | Most comprehensive |
| **User Control** | Minimal | Moderate | Maximum |

## 📦 Chunking Strategies

### **Fixed Size Chunking**
- Split text into fixed-size chunks with overlap
- Best for: Uniform content, consistent chunk sizes
- Parameters: Chunk size (50-20000 chars), Overlap (0-500 chars)

### **Recursive Chunking**
- Smart splitting that respects word boundaries
- Best for: Natural language text, preserving context
- Parameters: Chunk size, Overlap, Recursive separators

### **Semantic Clustering**
- Group similar content using embeddings
- Best for: Thematically related content, topic-based grouping
- Parameters: Number of clusters (2-max rows)

### **Document-Based Chunking**
- Group by key columns with token limits
- Best for: Structured data, preserving relationships
- Parameters: Key column, Token limit (200-10000), Headers option
- **Config-1**: Headers NOT included in chunks
- **Deep Config**: Headers optional (user choice)

## 🤖 Embedding Models

### **Local Models**
- **all-MiniLM-L6-v2**: Fast, good quality, 384 dimensions
- **paraphrase-MiniLM-L6-v2**: Best for semantic similarity, 384 dimensions

### **OpenAI Models**
- **text-embedding-ada-002**: High quality, 1536 dimensions
- **text-embedding-3-small**: Improved performance, 1536 dimensions
- **text-embedding-3-large**: Highest quality, 3072 dimensions

## 💾 Vector Storage

### **FAISS (Facebook AI Similarity Search)**
- **Pros**: Fast search, memory efficient, good for large datasets
- **Cons**: No metadata filtering, in-memory only
- **Best for**: Large datasets, fast retrieval
- **Metrics**: L2 (euclidean), Inner Product (dot), Cosine (with normalization)

### **ChromaDB**
- **Pros**: Persistent storage, metadata filtering, easy to use
- **Cons**: Slower than FAISS, more memory usage
- **Best for**: Metadata-rich data, persistent storage
- **Metrics**: cosine, ip (inner product), l2 (euclidean)

## 🗄️ Database Integration

### **Supported Databases**
- **MySQL**: Full support with connection testing
- **PostgreSQL**: Full support with connection testing
- **Features**: Table listing, data preview, large table handling

### **MySQL Setup**
```sql
CREATE DATABASE csv_chunker;
USE csv_chunker;
CREATE TABLE customers (
  id INT AUTO_INCREMENT PRIMARY KEY,
  name VARCHAR(100) NOT NULL,
  city VARCHAR(100),
  payment_method VARCHAR(50),
  amount DECIMAL(10,2)
);
```

### **PostgreSQL Setup**
```sql
CREATE DATABASE csv_chunker;
\c csv_chunker
CREATE TABLE customers (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100) NOT NULL,
  city VARCHAR(100),
  payment_method VARCHAR(50),
  amount NUMERIC(10,2)
);
```

## 📁 Project Structure

```
streamlit_chunking/
├── app.py                 # Streamlit frontend with deep config UI
├── main.py               # FastAPI backend with all endpoints
├── backend.py            # Core processing logic and enhanced functions
├── requirements.txt      # Python dependencies
├── README.md            # This documentation
├── current_state.pkl    # Global state persistence
├── chromadb_store/      # ChromaDB persistent storage (auto-created)
├── faiss_store/         # FAISS index storage (auto-created)
└── temp_files/          # Temporary file storage (auto-created)
```

## 🔧 API Endpoints

### **Processing Endpoints**
- `POST /run_fast` - Fast mode processing
- `POST /run_config1` - Config-1 mode processing
- `POST /run_deep_config` - Deep config mode processing

### **Deep Config Step-by-Step Endpoints**
- `POST /deep_config/preprocess` - Step 1: Load and preprocess data
- `POST /deep_config/type_convert` - Step 2: Convert data types
- `POST /deep_config/null_handle` - Step 3: Handle null values
- `POST /deep_config/stopwords` - Step 4: Remove stop words
- `POST /deep_config/normalize` - Step 5: Text normalization
- `POST /deep_config/chunk` - Step 6: Chunk data
- `POST /deep_config/embed` - Step 7: Generate embeddings
- `POST /deep_config/store` - Step 8: Store embeddings

### **Download Endpoints**
- `GET /deep_config/export/preprocessed` - Download preprocessed CSV
- `GET /deep_config/export/chunks` - Download chunks CSV
- `GET /deep_config/export/embeddings` - Download embeddings JSON

### **Database Endpoints**
- `POST /db/test_connection` - Test database connection
- `POST /db/list_tables` - Get table list
- `POST /db/import_one` - Import single table

### **Search & Export Endpoints**
- `POST /retrieve` - Semantic search
- `GET /export/chunks` - Download chunks
- `GET /export/embeddings` - Download embeddings

### **System Endpoints**
- `GET /system_info` - System memory usage
- `GET /file_info` - File information
- `GET /health` - Health check
- `GET /capabilities` - System capabilities

## 🚨 Troubleshooting

### **Common Issues**

1. **Import Error: mysql.connector**
   ```bash
   pip install mysql-connector-python
   ```

2. **Import Error: psycopg2**
   ```bash
   pip install psycopg2-binary
   ```

3. **spaCy Model Not Found**
   ```bash
   python -m spacy download en_core_web_sm
   ```

4. **NLTK Data Missing**
   ```python
   import nltk
   nltk.download('punkt')
   nltk.download('popular')
   ```

5. **Connection Refused (Database)**
   - Check if MySQL/PostgreSQL service is running
   - Verify host, port, username, password
   - Ensure database exists

6. **Memory Issues with Large Files**
   - Use smaller files (< 100MB) for testing
   - Increase system RAM
   - Enable large file processing mode
   - Use batch processing for very large datasets

7. **"N/A chunks created" Error**
   - Ensure API endpoints are running
   - Check that preprocessing step completed successfully
   - Verify data is loaded before chunking

8. **"Unknown storage type: chromadb" Error**
   - This has been fixed in v3.0
   - ChromaDB storage now works correctly

### **Performance Optimization**

- **Small files (< 10MB)**: Any mode works well
- **Medium files (10-100MB)**: Fast or Config-1 mode
- **Large files (> 100MB)**: Enable large file processing
- **Very large files (> 1GB)**: Use disk streaming mode
- **Memory**: Ensure 2-4GB free RAM for processing

## 📈 File Size Limits

| File Size | Recommended Mode | Processing Time | Memory Usage |
|-----------|------------------|-----------------|--------------|
| < 10MB | Any mode | < 30 seconds | < 100MB |
| 10-100MB | Fast/Config-1 | 1-5 minutes | 200-500MB |
| 100MB-1GB | Deep Config | 5-15 minutes | 500MB-2GB |
| > 1GB | Deep Config + Large File | 15+ minutes | 2GB+ |

## 🎯 Use Cases

### **Document Processing**
- Legal documents, contracts, reports
- Research papers, articles, books
- Customer feedback, reviews, surveys

### **Data Analysis**
- CSV data preprocessing and analysis
- Database content search and retrieval
- Structured data chunking and embedding

### **Knowledge Management**
- Company knowledge base
- FAQ systems
- Content recommendation engines

### **Research & Development**
- Literature review and analysis
- Patent search and analysis
- Technical documentation processing

## 🔄 Recent Updates (v1.0)

### **Major Improvements**
- ✅ **Step-by-Step API Integration**: Deep Config now uses individual API endpoints for each step
- ✅ **Database Import**: Full MySQL/PostgreSQL support across all modes
- ✅ **Config-1 Enhancements**: Added document chunking, retrieval metrics, removed null handling
- ✅ **Download Formats**: Updated to CSV for chunks, JSON for embeddings
- ✅ **State Persistence**: Global state management across API calls
- ✅ **Error Fixes**: Resolved "N/A chunks" and storage type issues
- ✅ **UI Improvements**: Better error handling, progress tracking, intermediate downloads

### **Bug Fixes**
- ✅ Fixed "N/A chunks created" error
- ✅ Fixed "Unknown storage type: chromadb" error
- ✅ Fixed API parameter mismatches
- ✅ Fixed global variable declaration issues
- ✅ Fixed database import search functionality

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🆘 Support

For issues and questions:

1. Check the troubleshooting section above
2. Review API documentation at http://localhost:8000/docs
3. Create an issue in the repository
4. Check the [GitHub repository](https://github.com/shanttoosh/streamlit_chunking) for updates

## 🙏 Acknowledgments

- **Streamlit** for the amazing web framework
- **FastAPI** for the high-performance API framework
- **Sentence Transformers** for embedding models
- **FAISS** for efficient similarity search
- **ChromaDB** for vector database functionality
- **spaCy** for advanced NLP processing
- **NLTK** for text processing utilities

---

**Made with ❤️ for the data processing community**

For more information, visit: [https://github.com/shanttoosh/streamlit_chunking](https://github.com/shanttoosh/streamlit_chunking)