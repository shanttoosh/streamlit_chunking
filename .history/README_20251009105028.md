# Chunk Optimizer v1.0

Advanced text chunking and embedding system with multiple processing modes, built with FastAPI and Streamlit.

## 🚀 Features

### Processing Modes
- **⚡ Fast Mode**: Optimized processing with semantic clustering
- **⚙️ Config-1 Mode**: Configurable chunking methods and parameters
- **🔬 Deep Config Mode**: Step-by-step comprehensive workflow

### Core Capabilities
- **Multiple Chunking Methods**: Fixed, recursive, semantic, and document-based chunking
- **Embedding Generation**: Local models and OpenAI API support
- **Vector Storage**: FAISS and ChromaDB support
- **Semantic Search**: Advanced retrieval with metadata filtering
- **Database Integration**: MySQL, PostgreSQL, and SQLite support
- **Large File Processing**: Optimized for files up to 3GB+
- **Export Functionality**: Multiple export formats (CSV, JSON, text)
- **OpenAI Compatibility**: Compatible with OpenAI API standards

## 📁 Project Structure

```
chunking-batch-processing/
├── src/                          # Source code
│   ├── api/                      # FastAPI backend
│   │   ├── routes/               # API endpoints
│   │   │   ├── processing.py     # Processing endpoints
│   │   │   ├── database.py       # Database operations
│   │   │   ├── retrieval.py      # Semantic search
│   │   │   ├── export.py         # Export functionality
│   │   │   ├── openai.py         # OpenAI-compatible endpoints
│   │   │   └── health.py         # Health checks
│   │   ├── middleware.py          # Custom middleware
│   │   └── main.py               # FastAPI application
│   ├── core/                     # Core processing logic
│   │   ├── preprocessing.py      # Data preprocessing
│   │   ├── chunking.py           # Text chunking algorithms
│   │   ├── embedding.py          # Embedding generation
│   │   ├── storage.py            # Vector storage
│   │   ├── retrieval.py          # Semantic search
│   │   ├── database.py           # Database operations
│   │   └── pipelines.py         # Processing pipelines
│   ├── ui/                       # Streamlit frontend
│   │   ├── components/           # UI components
│   │   │   ├── sidebar.py        # Sidebar component
│   │   │   ├── file_upload.py    # File upload component
│   │   │   ├── database_config.py # Database configuration
│   │   │   ├── processing_config.py # Processing configuration
│   │   │   ├── results_display.py # Results display
│   │   │   └── deep_config.py    # Deep config workflow
│   │   ├── utils/                # UI utilities
│   │   │   ├── api_client.py     # API client
│   │   │   ├── session_state.py  # Session state management
│   │   │   └── styling.py        # UI styling and themes
│   │   └── app.py                # Main Streamlit app
│   └── config/                   # Configuration
│       ├── settings.py           # Application settings
│       └── logging.py            # Logging configuration
├── tests/                        # Test suite
│   ├── unit/                     # Unit tests
│   ├── integration/              # Integration tests
│   └── conftest.py               # Test configuration
├── docs/                         # Documentation
│   ├── README.md                 # Project documentation
│   ├── API.md                    # API documentation
│   └── USAGE.md                  # Usage guide
├── scripts/                      # Utility scripts
│   ├── setup.py                  # Setup script
│   └── cleanup.py                # Cleanup script
├── data/                         # Data directory
├── storage/                      # Storage directory
├── streamlit_app.py              # Standalone Streamlit app
├── run_api.py                    # API server runner
├── run_ui.py                     # UI runner
├── requirements.txt               # Python dependencies
├── pyproject.toml                 # Project configuration
├── docker-compose.yml             # Docker Compose configuration
├── Dockerfile                     # Docker configuration
└── .gitignore                     # Git ignore rules
```

## 🛠️ Installation

### Prerequisites
- Python 3.8+
- pip or conda

### Setup
1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd chunking-batch-processing
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install optional dependencies**
   ```bash
   # For enhanced text processing
   pip install beautifulsoup4 spacy
   python -m spacy download en_core_web_sm
   
   # For database support
   pip install mysql-connector-python psycopg2-binary
   ```

## 🚀 Quick Start

### Start the API Server
```bash
python run_api.py
```
The API will be available at `http://localhost:8001`

### Start the UI Application
```bash
python run_ui.py
```
The UI will be available at `http://localhost:8501`

### Using Docker
```bash
docker-compose up
```

## 📖 Usage

### Web Interface
1. Open `http://localhost:8501` in your browser
2. Select a processing mode:
   - **Fast Mode**: Quick processing with default settings
   - **Config-1 Mode**: Customizable chunking and embedding
   - **Deep Config Mode**: Step-by-step advanced workflow
3. Upload a CSV file or configure database connection
4. Configure processing options
5. Run processing and view results
6. Use semantic search to query your data

### API Usage

#### Process with Fast Mode
```bash
curl -X POST "http://localhost:8001/api/v1/run_fast" \
  -F "file=@data.csv" \
  -F "use_turbo=true" \
  -F "batch_size=256"
```

#### Semantic Search
```bash
curl -X POST "http://localhost:8001/api/v1/retrieve" \
  -F "query=your search query" \
  -F "k=5"
```

#### OpenAI-Compatible Embeddings
```bash
curl -X POST "http://localhost:8001/v1/embeddings" \
  -F "model=text-embedding-ada-002" \
  -F "input=your text"
```

## 🔧 Configuration

### Environment Variables
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8001

# UI Configuration
UI_PORT=8501

# OpenAI Configuration
OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=https://api.openai.com/v1

# Database Configuration
DB_HOST=localhost
DB_PORT=3306
DB_USERNAME=username
DB_PASSWORD=password
DB_NAME=database_name
```

### Processing Options
- **Chunking Methods**: Fixed, recursive, semantic, document-based
- **Embedding Models**: paraphrase-MiniLM-L6-v2, all-MiniLM-L6-v2, text-embedding-ada-002
- **Storage Options**: FAISS, ChromaDB
- **Retrieval Metrics**: Cosine, dot product, euclidean distance

## 📊 Supported File Formats

### Input Formats
- **CSV Files**: Comma-separated values
- **Database Tables**: MySQL, PostgreSQL, SQLite
- **Large Files**: Optimized for files up to 3GB+

### Output Formats
- **Chunks**: CSV format
- **Embeddings**: JSON format
- **Text Export**: Human-readable format
- **API Responses**: JSON format

## 🔍 API Endpoints

### Processing Endpoints
- `POST /api/v1/run_fast` - Fast mode processing
- `POST /api/v1/run_config1` - Config-1 mode processing
- `POST /api/v1/run_deep_config` - Deep config mode processing

### Database Endpoints
- `POST /api/v1/db/test_connection` - Test database connection
- `POST /api/v1/db/list_tables` - List database tables
- `POST /api/v1/db/import_one` - Import single table

### Retrieval Endpoints
- `POST /api/v1/retrieve` - Semantic search
- `POST /api/v1/retrieve_with_metadata` - Search with metadata filtering
- `GET /api/v1/system_info` - System information

### Export Endpoints
- `GET /api/v1/export/chunks` - Export chunks as CSV
- `GET /api/v1/export/embeddings` - Export embeddings as JSON
- `GET /api/v1/export/embeddings_text` - Export embeddings as text

### OpenAI-Compatible Endpoints
- `POST /v1/embeddings` - OpenAI-compatible embeddings
- `POST /v1/chat/completions` - OpenAI-compatible chat completions

## 🧪 Testing

### Run Tests
```bash
# Run all tests
pytest

# Run unit tests
pytest tests/unit/

# Run integration tests
pytest tests/integration/

# Run with coverage
pytest --cov=src tests/
```

### Test Structure
- **Unit Tests**: Test individual components
- **Integration Tests**: Test API endpoints and workflows
- **Performance Tests**: Test with large datasets

## 🐳 Docker Support

### Build and Run
```bash
# Build image
docker build -t chunk-optimizer .

# Run container
docker run -p 8001:8001 -p 8501:8501 chunk-optimizer

# Using Docker Compose
docker-compose up -d
```

### Docker Compose Services
- **API**: FastAPI backend service
- **UI**: Streamlit frontend service
- **Database**: Optional database service

## 📈 Performance

### Optimization Features
- **Parallel Processing**: Multi-threaded embedding generation
- **Batch Processing**: Efficient batch operations
- **Memory Management**: Optimized for large files
- **Caching**: Intelligent caching system
- **Turbo Mode**: High-performance processing mode

### Performance Metrics
- **Processing Speed**: Up to 10x faster with turbo mode
- **Memory Usage**: Optimized for files up to 3GB+
- **Throughput**: 1000+ chunks per second
- **Latency**: Sub-second response times

## 🔒 Security

### Security Features
- **Input Validation**: Comprehensive input sanitization
- **Error Handling**: Graceful error handling
- **API Security**: CORS and authentication support
- **Data Privacy**: Local processing options
- **Secure Storage**: Encrypted storage options

## 🤝 Contributing

### Development Setup
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

### Code Style
- Follow PEP 8 guidelines
- Use type hints
- Write comprehensive docstrings
- Add unit tests for new features

## 📝 License

This project is licensed under the MIT License - see the LICENSE file for details.

## 🙏 Acknowledgments

- **FastAPI**: Modern, fast web framework
- **Streamlit**: Rapid application development
- **Sentence Transformers**: State-of-the-art embeddings
- **FAISS**: Efficient similarity search
- **ChromaDB**: Vector database
- **LangChain**: Text processing utilities

## 📞 Support

### Documentation
- **API Documentation**: Available at `/docs` when running the API
- **Usage Guide**: See `docs/USAGE.md`
- **API Reference**: See `docs/API.md`

### Issues
- **Bug Reports**: Use GitHub Issues
- **Feature Requests**: Use GitHub Discussions
- **Questions**: Use GitHub Discussions

### Contact
- **Email**: support@chunkoptimizer.com
- **GitHub**: [Repository Link]
- **Documentation**: [Documentation Link]

## 🔄 Changelog

### v2.0.0 (Current)
- **New**: Modular architecture with clean separation of concerns
- **New**: Enhanced UI with dark theme and improved UX
- **New**: Comprehensive API with OpenAI compatibility
- **New**: Advanced chunking algorithms
- **New**: Metadata filtering and enhanced retrieval
- **New**: Docker support and containerization
- **New**: Comprehensive test suite
- **New**: Performance optimizations for large files
- **Improved**: Better error handling and logging
- **Improved**: Enhanced documentation and examples

### v1.0.0 (Legacy)
- Initial release with basic functionality
- Fast and Config-1 processing modes
- Basic chunking and embedding
- Simple web interface

---

**Chunk Optimizer v1.0** - Advanced text processing and embedding system

Built with ❤️ using FastAPI, Streamlit, and modern Python technologies.